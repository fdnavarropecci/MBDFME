---
title: 'Master en Big Data. Fundamentos  matemáticos  del  análisis  de  datos.' 
author: "Fernando San Segundo"
date: 'Curso 2019-20. Última actualización: `r format(Sys.time(), "%Y-%m-%d")`'
subtitle: "Tema 7: Modelos. BORRADOR"
fontsize: 8pt
output:
  beamer_presentation:
    toc: true
    keep_tex: false
    includes:
      #after_body: afterbody.txt
      in_header: beamer-header-simple.txt
    colortheme: seahorse
    incremental: no
    slide_level: 2
    theme: Boadilla
bibliography: MBDFME.bib
---

```{r set-options, echo=FALSE, purl=FALSE, warning=FALSE, message=FALSE}
options(width = 60)
library(knitr)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

# Extendiendo el modelo lineal.

## Regresión simple con una variable explicativa más allá de las rectas.

+ Recuerda que antes hemos visto un conjunto de datos en el que el ajuste lineal mediante una recta resulta inadecuado. 
  ```{r echo=FALSE, message=FALSE, fig.align='center', out.width = "70%"}
  set.seed(2017)
  n = 100
  x = sort(runif(n))
  y1 = 3 + 2 * x + 0.3 * rnorm(n) # para que la muestra sea la misma que antes
  y =  x * (1 - x) + 0.01 * rnorm(n)
  plot(x, y, col="seagreen", pch=19, xlab="", ylab="")
  ```
  Lo razonable en un caso como este es ajustar una parábola o una curva simular a los datos. En R es muy fácil ajustar curvas de grado más alto con `lm`. Por ejemplo, para una parábola (polinomio de grado 2):\small
  ```{r echo=2}
  modeloParabola1 = lm(y ~ I(x) + I(x^2))
  modeloParabola = lm(y ~ poly(x, 2))
  xParabola = seq(min(x), max(x), length.out = 100)
  yParabola = predict(modeloParabola, 
                      newdata = data.frame(x = xParabola ))
  ```
  ```{r echo=FALSE, results='hide', purl = FALSE}
  texto = paste0(signif(modeloParabola$coefficients, c(4, 7, 7)), c("+  ", "x ", " x^2"), 
               collapse = "")
  ```

## El modelo ya no es una recta.

+ Añadimos esa parábola al diagrama de dispersión para que quede claro que ya no estamos ajustando rectas.\small
  ```{r echo=FALSE, message=FALSE, fig.align='center', out.width = "45%"}
  plot(x, y, col="seagreen", pch=19, xlab="", ylab="")
  lines(xParabola, yParabola, lwd= 3, col= "red")
  yP = predict(modeloParabola, newdata = data.frame(x = 0.7))
  points(0.7, yP, pch = 19, cex=3.5, col="orange")
  ```
  \normalsize Aunque no veamos la ecuación de la parábola, todo funciona casi igual. Por ejemplo podemos predecir valores con `predict`; el punto naranja que aparece destacado corresponde a $x= 0.7$ y el valor de $y$ correspondiente se ha obtenido con (ver código):\small
  ```{r eval=FALSE, message=FALSE, purl=FALSE, comment=NULL}
  predict(modeloParabola, newdata = data.frame(x = 0.7))
  ```
  \normalsize
  **Advertencia:**  a veces tenemos la tentación de usar un polinomio de grado tres, cuatro, etc. para tratar de *ajustar mejor* los datos. El riesgo de sobreajuste (overfitting) debe estar siempre presente en nuestro trabajo.\small
  
  *Nota:* Volviendo sobre la ecuación de la parábola, la interpretación de los coeficientes del modelo no es ahora tan sencilla como en la recta. Por razones técnicas sobre las que volveremos después, R usa *polinomios ortogonales* para hacer el ajuste. Si en el futuro usas mucho la regresión polinómica tendrás que entrar en estos detalles técnicos.\normalsize

## Dibujo con ggplot.

+ Se puede obtener un dibujo similar con ggplot así:\small
  ```{r message=FALSE, fig.align='center', out.width = "60%"}
  library(tidyverse)
  datos = data.frame(x, y)
  ggplot(datos) + 
    geom_point(aes(x, y)) + 
    geom_smooth(aes(x, y), method="lm", formula = y ~ poly(x, 2))
  ```
  \normalsize

## Ajuste de curvas exponenciales, logarítmicas, etc.

+ El tipo de curvas que se pueden usar no se agota en los polinomios, desde luego. A veces la curva que mejor se ajusta a unos datos es una exponencial, un logaritmo, etc. Exsiten también las llamadas técnicas de *ajuste local* (loess, ver \link{https://en.wikipedia.org/wiki/Local_regression}{Wikipedia}), que es el que utiliza por defecto `ggplot` en `geom_smooth` para dibujar curvas de tendencia y sus bandas de confianza, como en esta figura.\scriptsize
  ```{r message=FALSE, fig.align='center', out.width = "55%"}
  ggplot(mpg) + 
    geom_point(aes(displ, cty)) + 
    geom_smooth(aes(displ, cty))
  ```
  \normalsize Siempre hay que ejercer el sentido común a la hora de ajsutar curvas a los datos, para no caer en alguna de las situaciones sobre las que ironiza XKCD en las viñetas de la próxima página.

---

```{r echo=FALSE, eval=FALSE, purl=FALSE}
library(RXKCD)
getXKCD(which = "2048", saveImg = TRUE)
```
  ```{r echo=FALSE, message=FALSE, fig.align='center', out.width = "50%", purl=FALSE}
  include_graphics("../fig/xkcd_Curve-Fitting.png")
  ```

## Regresión multivariable.

+ El modelo de regresión lineal simple con una variable que hemos usado es 
  $$Y = \beta_0 + \beta_1 X + \epsilon,\qquad\text{ con }\quad\epsilon\sim N(0, \sigma)$$
  pero muchas veces vamos a querer estudiar situaciones en que $Y$ depende de más de ua variable explicativa. Como ejemplo usaremos una tabla con datos sobre pesos, alturas y edades de un grupo de niños. Inicialmente pensamos en la relación entre edad y peso.\scriptsize
  ```{r message=FALSE, fig.align='center', out.width = "60%"}
  childData = data.frame(
  wgt = c(64, 71, 53, 67, 55, 58, 77, 57, 56, 51, 76, 68), 
  hgt = c(57, 59, 49, 62, 51, 50, 55, 48, 42, 42, 61, 57), 
  age = c(8, 10, 6, 11, 8, 7, 10, 9, 10, 6, 12, 9))
  ggplot(childData) +
        geom_point(aes(age, wgt)) + 
        geom_smooth(aes(age, wgt), method="lm")
  ```

## Modelo de regresión lineal con una variable.

+ Como ya sabemos, poodemos analizar el modelo de la página anterior con R así:\small
  ```{r echo=FALSE}
    modelo1 = lm(wgt ~ age, data = childData)
    summary(modelo1)
  ```
  \normalsize Los dos asteriscos que aparecen en la fila `age` indican que parece haber una relación significativa entre edad y peso.
  
+ Pero al hacer esto no estamos teniendo en cuenta el efecto que la altura puede tener sobre esa relación. Como señala B. Caffo en \link{https://leanpub.com/regmods}{Regression Models} el objetivo principal de la regresión multivariable es *analizar la relación entre una variable explicativa y una respuesta, teniendo en cuenta el resto de las variables.*

## Modelo lineal con dos variables. 

+ La extensión del modelo lineal para incluir dos variables explicativas es formalmente muy sencilla:
  $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon,\qquad\text{ con }\quad\epsilon\sim N(0, \sigma)$$
  Es decir, añadimos un término más para la nueva variable.
  
+ Para obtener un modelo como este en R, en el que añadimos la variable altura `hgt` hacemos:\scriptsize
  ```{r}
      modelo2 = lm(wgt ~ age + hgt, data = childData)
      summary(modelo2)
  ```
  \normalsize Fíjate en que ahora la edad ya no aparece como significativa y la altura sí, aunque con un p-valor relativamente grande.
  
## Representación del modelo con dos variables. 

+ Al introducir dos variables explicativas en el modelo la geometría de la situación aumenta de dimensión. Ya no podemos representarla mediante un diagrama de dispersión en el plano, sino que tendríamos que ir a una representación tridimensional como esta:
  ```{r echo=FALSE, message=FALSE, fig.align='center', out.width = "45%", purl=FALSE}
  include_graphics("../fig/07-PlanoRegresion3d.png")
  ```
  Puedes explorar esta representación tridimensional en \link{}{este enlace}. El papel de la recta de regresión lo juega ahora un *plano de regresión*, pero las nociones de residuos, valores predichos, etc. siguen teniendo el mismo sentido.
  
+ Aunque vamos a usar ejemplos con dos variables explicativas, está claro que podríamos usar más variables y en esos casos ya no es posible visualizar el modelo así.


## Expresión de los coeficientes estimados en términos de residuos.

+ En el modelo de dos variables $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$ el valor predicho y el residuo para un par de valores de las variables predictoras $(x_{i1}, x_{i2})$ son:
$$
\hat y_i =  \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}, \qquad e_i = y_i - \hat y_i
$$
+ Hay una expresión interesante de los *coeficientes estimados* del modelo con dos variables, que facilita su interpretación. Por ejemplo para la estimación de $\beta_1$ es:
\small
$$
\hat\beta1 = \dfrac{\sum e_i(Y \sim X_2)e_i(X_1 \sim X_2)}{\sum (e_i(X_1 \sim X_2))^2}
$$
\normalsize
donde $e_i(Y \sim X_2)$ representa los residuos del modelo en el que solo usamos $X_2$ como variable explicativa, mientras $e_i(X_1 \sim X_2)$ son los residuos de un modelo en el usamos $X_2$ para explicar los valores de $X_1$. Al considerar los residuos con respecto a $X_2$ es como si *restáramos* el efecto de esa variable por un lado sobre $Y$ y por otro lado sobre $X_1$. Así que lo que queda en la estimación es el efecto de $X_1$ sobre $Y$ *ajustado* respecto de $X_2$.

+ En general en un modelo con $p$ variables explicativas $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \epsilon$ tendríamos:\small
$$
\hat\beta1 = \dfrac{\sum e_i(Y \sim (X_2+\cdots+ X_p)e_i(X_1 \sim (X_2+\cdots+ X_p))}{
\sum (e_i(X_1 \sim (X_2+\cdots+ X_p)))^2}
$$
\normalsize con expresiones similares para los otros $\hat\beta_i$. El caso de $\hat\beta_0$ se puede tratar introduciendo una variable auxiliar $X_0$ que vale 1 en todas las observaciones. 

## Comprobación con R en los datos del ejemplo. Identidad Anova.

+ La estimación de  $\beta1$, el coeficiente de `age` para el `modelo2` que es `wgt ~ age + hgt` es:\small
  ```{r}
  modelo2$coefficients
  ```
  \normalsize Para comprobar las expresiones residuales anteriores vamos a crear dos modelos auxiliares en los que analizamos el efecto de `hgt` sobre `wgt` y sobre `age` por separado:\small
  ```{r}
  modelo_yx2 =  lm(wgt ~ hgt, data = childData)
  modelo_x1x2 =  lm(age ~ hgt, data = childData)
  ```
  \normalsize y ahora usamos la expresión de $\hat\beta_1$ en términos de los residuos de estos dos modelos:\small
  ```{r}
  sum(residuals(modelo_yx2) * residuals(modelo_x1x2)) / sum(residuals(modelo_x1x2)^2)
  ```
  \normalsize Como puede verse, es el mismo resultado. 
  
+ Por otra parte, para estos modelos multivariable sigue siendo verdad la **identidad Anova** que ya vimos en el caso de una variable:
    $$
    \underbrace{\displaystyle\sum_{i=1}^{n}(y_i-\bar y)^2 }_{SStotal}=
    \underbrace{\sum_{i=1}^n e_i^2}_{SSresidual} + 
    \underbrace{\sum_{i=1}^n(\hat y_i-\bar y)^2}_{SSmodelo}
    $$
    y los términos que aparecen en ella se interpretan exactamente igual.
    
    
    
## Interpretación de los coeficientes.

+ Las expresión del modelo de regresión múliple permite interpretar de forma sencilla los coeficientes $\beta_i$. Por ejemplo $\beta_1$ es el incremento medio en el valor de $Y$ esperado al aumentar en una unidad el valor de $X_1$ *manteniendo el resto de las variables explicativas constantes (controlando respecto a esas variables)*.    

+ En el ejemplo la edad está en años, la altura en pulgadas y el peso en libras. Así que por ejemplo el valor estimado $\hat\beta_2\approx$ `r signif(modelo2$coefficients[3] ,4)` significa que por cada pulgada adicional de altura *pero manteniendo la edad constante* el peso aumenta en `r signif(modelo2$coefficients[3] ,4)` libras.

+ Vamos a comprobarlo usando predict. Creamos una tabla de tres datos de entrada con edad constante y alturas espaciadas por una pulgada:\small
  ```{r}
  nuevosDatos = data.frame(age = c(9, 9, 9), hgt = c(52, 53, 54))
  (pesosPredichos = predict(modelo2, newdata = nuevosDatos))
  ```
  \normalsize Y ahora vamos a ver las diferencias entre elementos sucesivos del vector de resultados (eso es precisamente lo que hace la función `diff`)\small
  ```{r}
  diff(pesosPredichos)
  ```
  \normalsize Las diferencias son precisamente el valor de $\hat\beta_2$, como esperábamos.
  
## Temas para seguir avanzando.

+ Apenas hemos rozado la superficie de los modelos de regresión múltiple y nos dejamos pendientes muchos aspectos, de los que aquí vamos a destacar solo algunos por su especial relevancia.

+ No hemos discutido la inferencia para este tipo de modelos, aunque es una generalización natural de lo que vimos en el caso de una variable. En particular con `lm` es sencillo obtener e interpretar inferencias tales como intervalos de confianza para los $\beta_i$, intervalos de predicción para valores de $Y$, contrastes de hipótesis sobre esos coeficientes, etc. Como muestra:\scriptsize
  ```{r}
  confint(modelo2)
  ```
  \normalsize

+ Para que esas inferencias estén bien fundadas es necesario comprobar en primer lugar que se verifican las hipótesis del modelo. Y para ello se usan herramientas de diagnóstico análogas a las que vimos en el caso de una variable: representaciones gráficas de los residuos que sirven para analizar la independencia, homogeneidad de la varianza, posibles 
puntos influyentes, etc.

+ Otro tema interesante del que aun no hemos tenido ocasión de tratar es la posible presencia de *interacción* entre las variables predictoras. Pero como lo vamos a discutir en el caso de Anova, dejamos esto aparcado por el momento.

+ Para profundizar en todos estos aspectos recomendamos consultar las fuentes que aparecen en la sección de Referencias. El tema seguramente más relevante por novedoso de los que nos quedan por tratar es el tema de la selección de modelos. Lo discutiremos a continuación.

## Selección de modelos. 

+ ¿Qué modelo es mejor para representar las relaciones entre variables en `childData`? ¿El modelo inicial `wgt ~ age` o el modelo con dos predictores `wgt ~ age + hgt`? Una manera de responder consiste en comparar la fracción de la variabilidad total (recuerda $SStotal$) explicada por cada uno de los dos modelos. Esto se lleva a cabo mediante una tabla Anova, que en R puede obtenerse mediante:\small
  ```{r}
  anova(modelo2)
  ```
  \normalsize Podemos interpretar esta tabla así: los tres asteriscos de la primera fila muestran que el modelo1 `wgt ~ age` es preferible al *modelo nulo* (que no usa variables explicativas y predice un valor constante `mean(wgt)`). A continuación el asterisco de la segunda fila dice que, con un p-valor mayor en este caso, el modelo2 `wgt ~ age + hgt` es preferible al modelo1. 

+ **Advertencias:** aunque pueda parecer igual, para R no es lo mismo `wgt ~ age + hgt` que `wgt ~ hgt + age`. Prueba a definir un modelo3 con `wgt ~ hgt + age` y luego haz `anova(modelo3)` para ver la diferencia. Además, en este caso las cosas sencillas porque los modelos están *anidados* (uno se obtiene del otro añadiendo una variable). Pero en el caso general la selección de modelos es *mucho más complicada*.

```{r echo=FALSE, eval=FALSE}
aov(modelo1)
(summ2 = summary(modelo2))
(aov2 = anova(modelo2))

modelo3 = lm(formula = wgt ~ hgt + age, data = childData)
(aov3 = anova(modelo3))

sum(aov2$`Sum Sq`[1:2]) / sum(aov2$`Sum Sq`)
summ2$r.squared
```

# Modelos lineales con factores. Anova. 

## Un ejemplo

+ Aunque la motivación inicial del modelo de regresión lineal está en la relación $C \sim C$ entre dos variables continuas, los métodos que se desarrollaron allí se pueden extender a otro tipo de situaciones. En particular es fácil extenderlos al caso $C \sim F$ en el que la variable respuesta es una variable continua pero queremos utilizar como variable explicativa un factor. 

+ Para centrar las ideas vamos a empezar por un ejemplo muy sencillo. Usando la tabla `iris` vamos a estudiar la relación entre la variable respuesta continua `Sepal.Length` y el factor `Species`.  Empezamos con una figura de boxplots por nivel que ilustra esa relación.\small
  ```{r message=FALSE, fig.align='center', out.width = "60%"}
  ggplot(iris) +
    geom_boxplot(aes(x = Species, y = Sepal.Length, color=Species))
  ```
  \normalsize

## El modelo lineal para $C\sim F$. 

+ Para describir una relación de tipo $C\sim F$ con notación similar a la de la regresión vamos a seguir llamando $Y$ a la variable respuesta (en el ejemplo la variable continua `Sepal.Length`). Para la variable predictora (el factor `Species`) vamos a hacer algo un poco más complicado. Puesto que la variable tiene tres niveles, vamos a usar dos *variables índice* auxiliares (en inglés se las denomina con poco acierto *dummy variables*), a las que llamaremos $X_1$ y $X_2$. Esas variables sólo toman los valores 0 y 1 y se tiene esta tabla de valores:
$$
\begin{array}{lccc}
& & \multicolumn{2}{r}{Variables}\\
  \cline{3-4}
  &\text{Species}& \multicolumn{1}{|c|}{X_1} &\multicolumn{1}{|c|}{X_2}\\
  \cline{2-4}
  &\multicolumn{1}{|c|}{\text{setosa}}&\multicolumn{1}{|c|}{0}&\multicolumn{1}{|c|}{0}\\
  \cline{2-4}
  &\multicolumn{1}{|c|}{\text{versicolor}}&\multicolumn{1}{|c|}{1}&\multicolumn{1}{|c|}{0}   \\
  \cline{2-4}
  &\multicolumn{1}{|c|}{\text{virginica}}&\multicolumn{1}{|c|}{0}& \multicolumn{1}{|c|}{1}\\
  \cline{2-4}
\end{array}
$$
+ Con estas variables la ecuación del modelo lineal te debería resultar familiar:
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon, \qquad \text{ con }\quad \epsilon\sim N(0, \sigma)
$$
enseguida volveremos sobre el significado de los $\beta_i$ de esta ecuación. 

+ ¡No te preocupes, R se encarga de todo esto automáticamente! Basta con escribir:
  ```{r}
  modelo = lm(Sepal.Length ~ Species, iris)
  ```
  y R se encarga de definir las variables auxiliares $X_1, X_2$ adecuadas. 

## Interpretación de los coeficientes del modelo.

+ El modelo que estamos construyendo predice como respuesta para cada nivel del factor $X$ la media de $Y$ en ese nivel del factor. Es decir, que si las medias de $Y =$ `Sepal.Length` en cada uno de los tres niveles de $X =$ `Species`  son, respectivamente
$\mu_1$, $\mu_2$ y $\mu_3$ entonces al usar la ecuación del modelo 
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon, \qquad \text{ con }\quad \epsilon\sim N(0, \sigma)
$$
con una observación del nivel $i$, debemos obtener como respuesta $Y = \mu_i$. 

+ Pero recuerda que si la observación es de la especie `setosa` entonces $X_1 = X_2 = 0$. Y al sustituir esto junto con $Y = \mu_1$ en la ecuación se obtiene  
$$\beta_0 = \mu_1$$
+ Si la observación es de la especie `versicolor` entonces $X_1 = 1,  X_2 = 0$. Sustituyendo esto con $Y = \mu_2$ en la ecuación vemos que ha de ser:  
$$\beta_1 = \mu_2 - \mu_1$$
De forma análoga, para una observación de la especie `virginica` se obtiene 
$$\beta_2 = \mu_3 - \mu_1$$

## Representación gráfica de los coeficientes del modelo.

+ Podemos visualizar esos coeficientes en un diagrama de boxplots paralelos mediante las flechas de colores que se muestran:
  ```{r echo=FALSE, message=FALSE, fig.align='center', out.width = "60%"}
  library(latex2exp)
  boxplot(Sepal.Length ~ Species, data = iris, ylim = c(0, 8), lwd = 1)
  medias = aggregate(Sepal.Length ~ Species, data = iris, FUN = mean)[,2]
  arrows(x0 = 1:3, y0 = c(0, medias[1], medias[1]), 
         x1 = 1:3, y1 = medias, col= c("red", "darkgreen", "blue"), lwd= 5)
  abline( h = medias[1], col="red", lty = 2, lwd = 4 )
  text(x=1, y= medias[1]/2, label=TeX("$\\beta_0 = \\mu_1$"), col="red", cex = 2)
  text(x=2, y= medias[1] - 0.5, label=TeX("$\\beta_1 = \\mu_2 - \\mu_1$"), col="darkgreen", cex = 2)
  text(x=3.1, y= medias[1] - 0.5, label=TeX("$\\beta_2 = \\mu_3 - \\mu_1$"), col="blue", cex = 2)
  ```
  Como indica el diagrama en este modelo la media $\mu_1$ del primer nivel (`setosa`) se usa como *nivel de referencia*; es decir, como término indepiente o *intercept* $\beta_0$ del modelo. Los dos coeficientes $beta_1 = \mu_2 - \mu_1$ y $\beta_2 = \mu_3 - \mu_1$ representan las diferencias entre las medias de esos niveles y el nivel de referencia. 

## Estimando los coeficientes del modelo. 

+ A poco que lo pensemos debería estar claro que la estimación de los valores $\mu_i$ se obtiene mediante las medias muestrales $\bar Y_1$, $\bar Y_2$ y $\bar Y_3$ de $Y$ en cada uno de los niveles del factor $X$. En R:\small
  ```{r}
  (medias = aggregate(Sepal.Length ~ Species, iris, FUN = mean)[,2])
  ```
  \normalsize Ahora calculamos $\bar y_1$, $\quad\bar y_2 - \bar y_1$,  $\quad\bar y_3 - \bar y_1$\small
  ```{r}
  c(medias[1], medias[2] - medias[1], medias[3] - medias[1])
  ```
  \normalsize   y comparamos con los coeficientes del modelo que se obtiene con `lm`:\small
  ```{r}
  modelo = lm(Sepal.Length ~ Species, iris)
  (coefs = modelo$coefficients)
  ```
  \normalsize que confirma nuestra interpretación de los coeficientes del modelo.
  
## Anova de una vía. Utilizando el modelo para hacer inferencia.

+ Un modelo lineal para $C\sim F$ como el que acabamos de describir se conoce clásicamente como *modelo Anova de una vía (one-way Anova)*. La palabra Anova se debe a que también en este caso se tiene una identidad Anova similar a la que vimos en la regresión. Si definimos el *residuo* de una observación como la diferencia $e_i = y_i - \hat y_i$ entre el valor observado y la estimación del valor predicho por el modelo, esa relación Anova se escribe de hecho exactamente igual:
    $$
    \underbrace{\displaystyle\sum_{i=1}^{n}(y_i-\bar y)^2 }_{SStotal}=
    \underbrace{\sum_{i=1}^n e_i^2}_{SSresidual} + 
    \underbrace{\sum_{i=1}^n(\hat y_i-\bar y)^2}_{SSmodelo}
    $$
  Lo único que cambia es la interpretación; aquí la estimación del valor predicho  $\hat y_i$ es la media muestral $\bar y_i$ correspondiente a un nivel del factor predictor $X$. A partir de esta relación se puede proceder de manera muy parecida a como lo hacíamos en el caso de la regresión.
  
+ El modelo que estamos usando es el que se emplearía por ejemplo para hacer un contraste de la hipótesis nula de que no hay diferencia entre las medias de los tres niveles:
$$
H_0 = \{\mu_1 = \mu_2 = \mu_3\}
$$
*¡Atención!* La hipótesis alternativa aquí no es "las tres medias son diferentes" sino "al menos hay dos medias que son diferentes."

## Información sobre el modelo con R.

+ Si hacemos:\scriptsize
  ```{r}
  (sumModelo = summary(modelo))
  ```
  \normalsize obtenemos mucha información sobre este modelo.  Queremos destacar el valor del coeficiente de correlación  `r signif(sumModelo$r.squared, 4)` que nos dice que el modelo con el factor `Species` explica el porcentaje correspondiente de la variabilidad total en `Sepal.Length`. 
  
## De nuevo, esto es solo el principio. 

+ Como dijimos en el caso de la regresión multivariable, apenas nos hemos asomado a la puerta de los modelos lineales. Todos los temas que adelantamos allí están presentes también cuando las variables explicativas (o algunas de ellas) son factores. 

+ En particular, debemos ocuparnos de la inferencia sobre los coeficientes del modelo, de verificar que se cumplen las hipótesis del modelo (diagnósticos del modelo), etc.

+ La formulación del modelo que hemos empleado (que es la que R usa por defecto) es adecuada para la hipótesis nula de igualdad de medias que hemos discutido antes. Supongamos que rechazamos esa hipótesis. Eso significa que al menos hay dos medias diferentes. La pregunta es evidente ¿cualés? Una manera de hacer esto es comparar dos a dos las medias de cada uno de los niveles. Cuando hay tres niveles, eso no es un problema. Pero si el factor tuviera, por ejemplo, 8 niveles, entonces el número de comparaciones dos a dos sería `r choose(8, 2)`. Y ya hemos visto que hacer tantas comparaciones puede producir errores de tipo I por puro azar. 

+ Además, es fácil comprender que existen extensiones naturales de estos modelos a situaciones en las que interviene más de un factor como variable explicativa. En ese tipo de modelos (de doble vía si intervienen dos factores, etc.) el análisis es mucho más delicado y choca de lleno con el tema del Diseño Experimental, del que no hemos hablado. 



<!-- ##  Simpson's -->

<!-- ```{r} -->
<!-- library(tidyverse) -->
<!-- n = 100 -->
<!-- x2 <- 1 : n -->
<!-- x1 = 0.1 * x2 + 4 * runif(n, -.1, .1) -->
<!-- y = - x1 + x2 + rnorm(n, sd = .01) -->
<!-- summary(lm(y ~ x1))$coef -->
<!-- summary(lm(y ~ x1 + x2))$coef -->
<!-- summary(lm(x1 ~ x2)) -->
<!-- datos = data.frame(x1, x2, y) -->
<!-- colores = as.integer(cut(x2, breaks = 10)) -->
<!-- ggplot(datos) + -->
<!--   geom_point(aes(x1, y), color = heat.colors(10)[colores]) -->
<!-- ``` -->


# Modelos lineales generalizados (glm). Regresión Logística. 

## Relaciones del tipo $F\sim C$ para factores binarios.

+ En este apartado vamos a estudiar un modelo adecuado para relaciones del tipo $F ~ C$, en las que usamos una variable continua $X$ como predictor de una variable respuesta $Y$ de tipo factor, que inicialmente suponemos binario (con dos niveles).

+ Para guiarnos en la discusión vamos a utilizar un conjunto de datos disponible en el   
\link{http://wiley.mpstechnologies.com/wiley/BOBContent/searchLPBobContent.do}{sitio web de la editorial Wiley} y procedente del libro *Applied Logistic Regression* de S. Lemeshow [@hosmer2013applied]. Introduce en el campo de búsqueda adecuado el ISBN del libro que es: 9780470582473 haz clic en *Search* y después haz clic en el enlace con ese número que aparecerá . A continuación marca la casilla para seleccionar todos los ficheros y después haz clic en *Download* como indica la figura. Descargarás entonces un fichero zip con todos los ficheros de datos necesarios. Nosotros vamos a usar uno de los ficheros que contiene, llamado `CHDAGE.txt` con datos sobre la existencia de enfermedad coronaria en un grupo de pacientes. Asegúrate de colocar ese fichero en la subcarpeta `datos` de tu directorio de trabajo.  
  ```{r echo=FALSE, message=FALSE, fig.align='center', out.width = "35%", purl=FALSE}
  include_graphics("../fig/Wiley_Landing_Page-02.png")
  ```
  
## Descripción del problema.

+ Después de leer los datos a un data.frame de R que vamos a llamar CHDdata, lo exploramos:\small
  ```{r}
  CHDdata = read.table("../datos/CHDAGE.txt", header = TRUE)
  head(CHDdata)
  ```
  \normalsize Como puede verse hay tres variables: `ID` es simplemente un identificador y no la usaremos; `AGE` es la edad con valores enteros y `CHD` (de *coronary heart disease*) es un factor codificado como una variable binario que toma los valores 0 o 1 para indicar, respectivamente, la ausencia o presencia de enfermedad coronaria. 

+ Recuerda que siempre conviene empezar explorando los datos. En este caso usamos `summary`:\scriptsize
  ```{r}
  summary(CHDdata)
  ```
  \normalsize En particular comprobamos que no hay datos ausentes.

## Representando los datos. 

+ Para entender lo que queremos hacer vamos a usar una representación gráfica similar al dotplot del que hablamos al principio del tema 6.\small
  ```{r message=FALSE, fig.align='center', out.width = "50%"}
  ggplot(CHDdata) + 
    geom_point(aes(x = AGE, y = CHD, size=2), 
               show.legend=FALSE, position = position_jitter(w = 0, h = 0.02)) +
    geom_hline(yintercept = 0:1, linetype = "dashed", color = "blue", size = 2)
  
  ```
  \normalsize Aunque $Y$ es binario hemos *"agitado"* los puntos verticalmente para mejorar la visualización. Fíjate en que a medida que $X = AGE$ aumenta hay más valores de $Y = CHD$ iguales a 1. Como cabría esperar, la presencia de enfermedades coronarias aumenta con la edad. Esa es la *tendencia o señal* que estamos tratando de detectar o cuantificar expresándola a través de algún tipo de modelo.   

## Construcción del modelo.

+ En una situación como esta no podemos usar un modelo lineal de regresión del tipo 
$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$ 
porque la respuesta $Y$ no es continua, *solo toma dos valores*. 

+ La idea astuta que nos va a permitir avanzar en este caso es una que ya nos hemos encontrado antes. Cuando tratamos con una variable continua podemos agruparla en clases (en R con `cut`) para tratarla como un factor ordenado. Definimos unas franjas de edad así:\small
  ```{r}
  (AGEbreaks = c(20, seq(from = 30, to = 60, by = 5), 70))
  ```
  \normalsize Los intervalos son de 5 en 5 años salvo el primero y el último porque tenemos pocos datos en los extremos. Usamos estos valores para cortar las edades y añadimos esa información a los datos:\small
  ```{r}
  CHDdata$AgeGroup = cut(CHDdata$AGE, breaks = AGEbreaks, right = FALSE)
  ```
  \normalsize Hagamos una tabla de contingencia de CHD frente al grupo de edad:\small
  ```{r echo=-1}
  options(width = 80)
  (tabla1 = as.matrix(table(CHDdata$CHD, CHDdata$AgeGroup)))
  ```
  \normalsize Ahí está de nuevo, visible, la señal. En la segunda fila de la tabla los valores aumentan claramente de izquierda a derecha (y en la primera ocurre al revés).
  
## Pensando en términos de probabilidades.

+ Vamos a calcular la suma por columnas de esta tabla (es una tabla de frecuencias):\small
  ```{r}
  (sumaColumnas = colSums(tabla1))
  ```
  \normalsize Y dividimos la segunda fila de la `tabla1` anterior por estas sumas (redondeada a dos cifras). :\small
  ```{r }
  (probs = signif(tabla1[2, ] / sumaColumnas, 2))
  ```
  \normalsize Esto permite pensar en los datos en términos de probabilidades. Por ejemplo, el porcentaje de pacientes de 45 a 50 años con enfermedad coronaria es el 46% y asciende al 76% de 55 a 60 años. ¡Esa es la idea clave! Vamos a añadir esto al gráfico (ver código de la sesión):\small
  ```{r echo=FALSE, message=FALSE, fig.align='center', out.width = "40%"}
  midpoints = AGEbreaks[-length(AGEbreaks)] + c(5, rep(2.5, 6), 5)
  probsdf = data.frame(midpoints, probs)
  ggplot(CHDdata) + 
    geom_point(aes(x = AGE, y = CHD, size=4), 
               show.legend=FALSE, position = position_jitter(w = 0, h = 0.02)) +
    geom_hline(yintercept = 0:1, linetype = "dashed", color="blue", size=2) + 
    geom_point(data = probsdf, 
               mapping = aes(x = midpoints, y = probs, size=4, col="red"),
               show.legend=FALSE)
  ```
  \normalsize Los puntos rojos indican la probabilidad de $Y = 1$ para cada intervalo de edades. 

## El modelo es una curva con forma de s.

\label{figuraCurvaLogistica}

+ Los puntos rojos de la gráfica previa insinuan una curva con forma de s (*curva sigmoidal*) de izquierda a derecha, como esta (puedes ver el código para ver como la hemos dibujado, pero solo se entenderá después de aprender un poco más):
  ```{r echo=FALSE, message=FALSE, fig.align='center', out.width = "60%"}
  glmCHD = glm(CHD ~ AGE, family = binomial(link = "logit"), CHDdata)
  summGlmCHD = summary(glmCHD)
  curvaX = data.frame(AGE = seq(20, 70, length.out = 101))
  curvaY = predict(glmCHD, newdata = curvaX, type = "response")
  curvaDf = data.frame(AGE = curvaX$AGE, CHD = curvaY)
  ggplot(CHDdata) + 
  geom_point(aes(x = AGE, y = CHD, size=4), 
             show.legend=FALSE, position = position_jitter(w = 0, h = 0.02)) +
    geom_hline(yintercept = 0:1, linetype = "dashed", color="blue", size=2) + 
    geom_line(data = curvaDf, aes(x = AGE, y = CHD, col= "red", size=3), 
              show.legend=FALSE)
  ```
  \normalsize Esa curva representa el modelo que buscábamos para este tipo de situaciones. Es muy importante recordar que la coordenada vertical de los puntos de esa curva son **probabilidades condicionadas**. Es decir, un punto $(x_0, p_{x_0})$ de esa curva representa:
$$p_0 = P(Y = 1 | X = x_0)$$

## Curvas logísticas. Ajuste mediante la verosimilitud.
 
+ Una vez entendido esto, el siguiente paso es más técnico. La familia de curvas sigmoidales que vamos a usar (puedes pensar en ellas como un sustituto de las rectas de regresión) es esta:
\begin{center}
\fcolorbox{black}{Gris025}{
\begin{minipage}{10cm}
\begin{center}
{\bf  Curvas logísticas.}
\end{center}
Dados dos números cualesquiera $\beta_0, \beta_1$ la curva logística correspondiente es:
$$
f(x) = \dfrac{e^{\beta_0+\beta_1 x}}{1+e^{\beta_0+\beta_1 x}}.
$$
\end{minipage}}
\end{center}
Puedes familiarizarte con las propiedades de esta familia de curvas en \link{https://www.geogebra.org/m/t82ap6eb}{este enlace}.

+ Ahora se trata de elegir los valores de $\beta_0$ y $\beta_1$ que producen *la mejor curva logística posible* para ajustarla a nuestros datos. Esto debe recordarte a lo que hicimos cuando buscábamos la mejor recta de regresión posible. Allí usamos el método de mínimos cuadrados, partiendo de una expresión de los residuos del modelo. Pero ahora no podemos hacer esto, por razones técnicas (la estructura de error del problema no sigue una distribución normal como ocurría en la regresión, sino una binomial). El método que se emplea es uno de los más importantes en Estadística, con un  alcance muy general. Se basa en la función **verosimilitud (likelihood)**. En un sentido muy amplio la verosimilitud de un modelo con ciertos parámetros (como $\beta_0, \beta_1$) se define como:
  $$
  \mathcal{L}(\mbox{modelo con parámetros}) = 
  P(\mbox{datos}\,|\,\mbox{dados los valores de los parámetros})
  $$
  y el método para determinar $\beta_0$ y $\beta_1$ en regresión logística consiste en elegir los que maximizan la función de verosimilitud (por razones técnicas se minimiza el logaritmo de $\cal L$, que se denomina *loglikelihood*).

## Ajustando un modelo logístico con R. Función `glm`.

+ La función verosimilitud es computacionalmente mucho más complicada que el error cuadrático medio que usábamos en la regresión. Así que no vamos a dar fórmulas para la estimación de $\beta_0$ y $\beta_1$ como hacíamos allí. Dejaremos que R se encargue de hacer la estimación por nosotros. Para ello se usa, en lugar de `lm`, la  función `glm` de *generalized linear models*. Volveremos después sobre este nombre. En nuestro ejemplo esto se hace con:\small
  ```{r}
  (glmCHD = glm(CHD ~ AGE, family = binomial, data = CHDdata))
  ```
  \normalsize El formato de la llamada a la función `glm` y de la salida es similar a lo que ya conocemos, salvo por el argumento `family = binomial` que es que le indica a `glm` que queremos un modelo logístico. En particular obtenemos *estimaciones* $\hat\beta_0$ y $\hat\beta_1$ de los parámetros del modelo logístico:\small
  ```{r}
  coefficients(glmCHD)
  ```
  \normalsize Ahora es un buen momento para volver a examinar el código que genera \hyperlink{figuraCurvaLogistica}{\textcolor{blue}{esta figura}} y tratar de entenderlo.  
 
## Usando el modelo logístico para predecir.

+ Con el modelo logístico y la función `predict` podemos hacer predicciones de forma muy parecida a lo que hacíamos en el caso de un modelo lineal. Por ejemplo, ¿cuál es la probabilidad de padecer una enfermedad coronaria (probabilidad de $Y =1$) que predice el modelo para un paciente de $X = 32$ años?\small
  ```{r}
  edadPredecir = data.frame(AGE = 32)
  (probCHD = predict(glmCHD, newdata = edadPredecir, type = 'response'))
  ```
  \normalsize es decir, un `r signif(100 * probCHD,2)`%. 
  
  El argumento `type = 'response'` que hemos incluido en la llamada a `predict` es el que permite obtener una probabilidad. Si hubiéramos usado `type = 'link'` habríamos obtenido como respuesta el valor
$$\hat\beta_0 + \hat\beta_1 x_0$$
donde, en este ejemplo, $x_0 = 32$. Compruébalo:\small
  ```{r}
  predict(glmCHD, newdata = edadPredecir, type = 'link')
  coefficients(glmCHD)[1] + coefficients(glmCHD)[2] * 32
  ```
  \normalsize El valor $\hat\beta_0 + \hat\beta_1 x_0$ se denomina *log-odds* de $x_0$.


## Odds y log-odds en el contexto de la regresión logística.

+ Es una forma de entender la probabilidad común en el mundo anglosajón y especialmente en el contexto de las apuestas deportivas. Si cuelves a pensar en la Regla de Laplace recordarás que era:
$$
P(A) = 
\dfrac{\text{núm. de sucesos elementales favorables a }A}{\text{núm. total de sucesos elementales}}.
$$
En la misma situación los odds de $A$ (a mi me gusta traducirlo por *psoibilidades*, pero na hay una versión establecida en español) se calculan como:\small
$$
O_A=
\dfrac{\text{núm. de sucesos elementales favorables a }A}
{\text{núm. de sucesos elementales {\bf contrarios} a }A}.
$$
\normalsize así que por ejemplo una probabilidad de $\frac{3}{11}$ se convierte fácilmente en unos odds de $\frac{3}{11 - 3} = \frac{3}{8}$ (apuestas 3 a 8). En general los odds correspondientes a una probabilidad $p$ son:\small
$$O_p = \dfrac{p}{1 - p}$$\normalsize

+ ¿Por qué es relvante esto en el contexto de la regresión logística? Pues porque el modelo se puede escribir
$$P(Y = 1 | X = x) = p = \dfrac{e^{\beta_0+\beta_1 x}}{1+e^{\beta_0+\beta_1 x}}.$$
y si llamamos $w = \beta_0+\beta_1 x$ y despejamos se obtiene:
$$w = \ln\left(\dfrac{p}{1 - p}\right) = \ln\left(O_p\right)$$
Por eso decimos que $\beta_0+\beta_1 x$ son los **log-odds** correspondientes a $x$.

## Interpretación del coeficiente $\beta_1$ del modelo de regresión logística. 

+ Ahora ya podemos interpretar $\beta_1$. Si aumentamos en una unidad el valor de $x$ entonces los log-odds $\beta_0+\beta_1 x$ aumentan precisamente en $\beta$. A partir de aquí podemos transformar este resultado para traducirlo en términos de probabilidades. 

+ Esto ayuda a entender porque `predict` ofrece la opción de obtener el resultado en términos de *log-odds*. En algunos contextos esa cantidad es preferible porque su relación matemática con la variable predictora $X$ es más simple. 

+ Es muy importante darse cuenta de que la relación sencilla de los valores de $X$ es con los log-odds, no con las probabilidades. Fíjate en lo que pasa cuando consideramos varios valores consecutivos de la edad y miramos los valores predichos de los log-odds y de las probabilidades:\scriptsize
  ```{r}
  edades = data.frame(AGE = 50:55)
  probabilidades = predict(glmCHD, newdata = edades, type = 'response')
  diff(probabilidades)
  logOdds = predict(glmCHD, newdata = edades, type = 'link')
  diff(logOdds)
  coefficients(glmCHD)[2]
  ```
  \normalsize Como puedes ver al aumentar la edad en un año el incremento de las probabilidades (calculado con `diff`) no es constante, pero el de los log-odds sí y coincide con el coeficiente estimado $\hat\beta_1$.

## La idea detrás del modelo lineal generalizado (glm).

+ La idea clave en la construcción del modelo de regresión logística de una variable es la de pasar de pensar en los valores de $Y$ a pensar en las probabilidades $p = P(Y = 1| X= x)$; *el modelo predice probabilidades*. Pero todavía se puede dar un paso más y mirar el modelo desde una perspectiva más general. 

+ Para hacer esto, debemos recordar que la *variable respuesta condicionada* $(Y|X= x)$ se puede entender como una variable binaria que toma los valores 1 (con probabilidad $p$) y 0 (con probabilidad $q$). Es decir, que es una variable de Bernouilli. Y la media de esa variable de Bernouilli es precisamente $\mu_{Y|X= x} = p$. Así que dando un paso más podemos pensar que en lugar de predecir la probabilidad *el modelo predice medias de $Y$* (una media para cada valor de $X$).

+ Pero si recordamos la expresión del modelo de regresión lineal: 
$$Y = \beta_0 + \beta_1 X + \epsilon,\qquad\text{ con }\quad\epsilon\sim N(0, \sigma)$$
al tomar la media de la variable respuesta condicionada $(Y| X= x)$ se obtiene, teniendo en cuenta que $\mu_{\epsilon} = 0$:
$$\mu_{Y|X= x} = \beta_0 + \beta_1 x = \hat Y(x)$$
es decir, la media $\mu_{Y|X= x}$ coincide con el valor que predice el modelo. Otra vez: *el modelo predice medias de $Y$* (una media para cada valor de $X$).

## Formalizando el glm.

+ Podemos convertr esa idea en una definición. Un **modelo lineal generalizado** consta de tres componentes:  
  
  $1.$ Una **función de distribución** de probabilidad $f$ con una distribución conocida para modelizar la variable respuesta condicionada. Técnicamente, la distribución $f$ debe ser de la *familia exponencial*, una familia muy amplia de variables aleatorias que incluye la normal, la binomial, etc.   
  $2.$ Un **predictor lineal** $\beta_0 + \beta_1 X_1 + \beta_2 X_2 +\cdots + \beta_p X_p$ que depende de las variables predictoras $X_1,\ldots, X_p$ (pueden ser continuas o factores usando variables índice).   
  $3.$ Una **función de enlace (link function)** $g$ que sirve para lo que esperamos del modelo: *predecir medias de $Y$* (una media para cada valor de $X$).  
   
+ *Ejemplo 1:* en el modelo de regresión lineal simple la distribución $f$ que usamos es la de normal $N(0, \sigma)$, el predictor lineal es $\beta_0 + \beta_1 X$ mientras que la función de enlace es la identidad $g(\mu) = \mu$. Por eso este modelo es el más simple, por la sencillez del enlace.

+ *Ejemplo 2:* en el modelo de regresión logística con una variable la distribución $f$ es la Bernouilli con probabilidad $p = \mu_{Y| X= x}$ que hemos visto, el predictor lineal vuelve a ser $\beta_0 + \beta_1 X_1$ mientras que la función de enlace es $g(\mu) = \log\left(\frac{\mu}{1 - \mu}\right)$, la función log-odds que es la inversa de la transformación logística. 

+ Se pueden definir otros modelos lineales generalizados cambiando estas componentes para por ejemplo modelizar una relación $Y \sim X$ donde $Y$ es una variable de tipo Poisson (regression de Poisson) o para modelizar una variable respuesta de tipo factor politómico con  más de dos niveles (regresión multinomial). Ver \link{https://en.wikipedia.org/wiki/Generalized\_linear\_model\#Count\_data}{Wikipedia} y \link{https://leanpub.com/regmods}{Regression Models} de B.Caffo.

# Complementos de R.

## Familia apply.

+ La función `apply` sirve para aplicar una función a una tabla *marginalmente* (por filas o por columnas). Tabla aquí significa una estructura tabular, como un data.frame o matriz. Por ejemplo, para calcular un intervalo de confianza para cada columna de una matriz:\scriptsize
  ```{r}
  options(width = 80)
  set.seed(2019)
  M = matrix(rnorm(100 * 5), ncol = 5)
  head(M)
  apply(M, MARGIN = 2, 
        FUN = function(x)t.test(x, alternative = "two.sided")$conf.int)
  ```
  \normalsize La función que hemos aplicado es una *función anónima*, definida ad hoc dentro de apply y el resultado es una matriz que contiene en cada columna el intervalo de confianza correspondiente.

+ **Notas:**  En los últimos años la implementación los bucles for de R han mejorado mucho y ya no puede decirse que `apply` sea mucho más rápido. Con `apply` se sonigue código más compacto, pero menos legible. La librería `dplyr`es una alternativa recomendable frente a muchos usos tradicionales de apply. Veremos a continuación las funciones más destacadas de la familia `apply`.

## lapply y sapply

+ La función `lapply` es similar, pero en este caso operando sobre listas. Es decir, `lapply` recorre los elementos de la lista, aplica una función a cada uno de ellos y produce como salida una lista con los resultados. Por ejemplo, aquí aplicaremos `lapply` para calcular la dimensión de los elementos de una lista que contiene data.frames, matrices o tablas.\scriptsize
  ```{r}
  L = list(A = iris, B = matrix(1:12, nrow = 3), 
           C = table(mpg$cyl), D = iris)
  lapply(L, FUN = dim)
  ```
  \normalsize

+ Por su parte `sapply` actúa como `lapply` pero trata de simplificar el resultado para producir como salida si es posible un vector o matriz en lugar de una lista. Una situación típica es cuando queremos aplicar una función a cada elemento de un vector. Por ejemplo, vamos a fabricar muestras de tamaño 4 de varias binomiales el mismo $p= 1/3$ pero con tamaño distinto:\scriptsize
  ```{r}
  (muestras = sapply(4:8, function(k)rbinom(n = 4, size = k, prob = 1/3 )))
  ```
  \normalsize Como se ve no obtenemos una lista de muestras sino una matriz.

## tapply

+ La función `tapply` solía usarse para aplicar funciones (como la media) a columnas de un data.frame según los valores de un factor en otra columna. Por ejemplo en la tabla `mpg` la media del consumo urbano en cada clase de vehículo se obtiene con:\scriptsize
  ```{r}
  tapply(mpg$cty, INDEX = mpg$class, FUN = mean)
  ```
  \normalsize aunque hay alternativas más claras como `aggregate`\scriptsize
  ```{r}
  aggregate(cty ~ class, data = mpg, FUN = mean)
  ```
  \normalsize o más preferiblemente `dplyr` con `group_by` y `summarize`.\scriptsize
  ```{r}
  mpg %>% group_by(class) %>% summarize(mean(cty))
  ```

  \normalsize
  
## Fechas en R. Lubridate.

+ Las fechas (y horas) son un tipo de dato muy frecuente pero a menudo difícil de gestionar, por los diferentes formatos que se usan. En el ecosistema del `tidyverse` disponemos de la librería `lubridate` para facilitar ese tipo de operaciones. Recomendamos la lectura del \link{https://r4ds.had.co.nz/dates-and-times.html}{Capítulo 16} de *R for Data Science* o el Capítulo 8 de [@Boehmke2016].

+ Una de las operaciones básicas es la conversión de una fecha en formato texto al tipo de datos fecha que usa `lubridate`. Existen diversas funciones de conversión para los formatos más habituales. En este ejemplo, después de examinar el texto hemos optado por la función `dmy_hm` (de *day-month-year_hour_minute*):\small
  ```{r message = FALSE}
  library(lubridate)
  fechaTexto = "21-07-1969 02:56UTC"
  (fecha = dmy_hm(fechaTexto))
  ```
  \normalsize De la misma forma, existen funciones como `ydm`, `hms`, etc. para extraer la información necesaria de la mayoría de formatos de texto que encontrarás. 

## Acceso a las componentes de una fecha y operaciones con fechas. 

+ Una vez que disponemos de una fecha en el formato correcto, también podemos extraer de ella las partes que la componen:\scriptsize
  ```{r}
  day(fecha)
  month(fecha)
  hour(fecha)
  ```
  \normalsize

+ También es muy fácil hacer operaciones con fechas. Por ejemplo, ¿qué fecha se obtiene si sumamos 100 días a la fecha de ejemplo que venimos usando?\small
  ```{r}
  fecha + days(100)
  ```
  \normalsize ¿Y si le sumamos 1000 horas y 42 minutos?\small
  ```{r}
  fecha + hours(1000)  + minutes(42)
  ```
  \normalsize El 29 de octubre de ese mismo año se envio el primer mensaje a través de Arpanet, el precursor de Internet. ¿Cuántos días separan ambas fechas?\small
  ```{r}
    fecha2 = dmy_hm("29-10-1969 00:00")
    fecha2 - fecha
  ```
\normalsize  




## Algunos recursos online para seguir aprendiendo R . 

**Cursos online (gratuitos y comerciales).**

+ Coursera & John Hopkins Data Science Specialization.
 
+ DataCamp, cursos de pago, impartidos en algunos casos por expertos como el propio Hadley Wickham.



## Referencias para la sesión

**Enlaces**

```{r eval=FALSE, echo=FALSE, purl=FALSE, message=FALSE, error=FALSE}
sessionName = "07-Modelos"
RmdName = paste0(sessionName,".Rmd")
ScriptName = paste0(sessionName,".R")
lnkScriptGitHub = paste0("https://raw.githubusercontent.com/fernandosansegundo/MBDFME/master/scripts/", ScriptName)
knitr::purl(RmdName, output = paste0("../scripts/", ScriptName))
```


**Bibliografía**

